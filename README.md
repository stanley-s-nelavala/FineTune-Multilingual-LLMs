# FineTune-Multilingual-LLMs

This project explores the multilingual capabilities of fine-tuned large language models (LLMs), specifically **LLAMA2** and **Mistral 7B**, across tasks such as text completion, question answering, and summarization.

## 📚 Description

The notebook provided demonstrates how to fine-tune LLaMA 2 and Mistral models using QLoRA on Google Colab. We use both multilingual and bilingual datasets including **Samanantar** (Telugu subset) and **OpenAssistant-Guanaco**, analyzing the effectiveness and performance of these models under constrained GPU resources.

## 📁 Structure

- `notebooks/` — Contains the Google Colab notebook used for training
- `docs/` — Final project report summarizing methodology and findings
- `scripts/` — (Optional placeholder for future scripts or utilities)
- `LICENSE` — MIT License

## 🧠 Key Methods

- Parameter-Efficient Fine-Tuning (PEFT)
- QLoRA (Quantized LoRA)
- Evaluation of multilingual NLP capabilities

## 🔗 Resources

- [Samanantar Dataset](https://ai4bharat.iitm.ac.in/samanantar)
- [OpenAssistant Guanaco](https://huggingface.co/datasets/OpenAssistant/oasst1)

