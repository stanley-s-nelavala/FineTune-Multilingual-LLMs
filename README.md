# FineTune-Multilingual-LLMs

This project explores the multilingual capabilities of fine-tuned large language models (LLMs), specifically **LLAMA2** and **Mistral 7B**, across tasks such as text completion, question answering, and summarization.

## ğŸ“š Description

The notebook provided demonstrates how to fine-tune LLaMA 2 and Mistral models using QLoRA on Google Colab. We use both multilingual and bilingual datasets including **Samanantar** (Telugu subset) and **OpenAssistant-Guanaco**, analyzing the effectiveness and performance of these models under constrained GPU resources.

## ğŸ“ Structure

- `notebooks/` â€” Contains the Google Colab notebook used for training
- `docs/` â€” Final project report summarizing methodology and findings
- `scripts/` â€” (Optional placeholder for future scripts or utilities)
- `LICENSE` â€” MIT License

## ğŸ§  Key Methods

- Parameter-Efficient Fine-Tuning (PEFT)
- QLoRA (Quantized LoRA)
- Evaluation of multilingual NLP capabilities

## ğŸ”— Resources

- [Samanantar Dataset](https://ai4bharat.iitm.ac.in/samanantar)
- [OpenAssistant Guanaco](https://huggingface.co/datasets/OpenAssistant/oasst1)

